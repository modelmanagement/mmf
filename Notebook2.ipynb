{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T09:48:03.571885Z",
     "start_time": "2019-03-06T09:48:03.383546Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e38642ca161467c9d69af9ca4b46686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MagicsControllerWidget(children=(Tab(children=(ManageSessionWidget(children=(HTML(value='<br/>'), HTML(value='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added endpoint http://10.34.14.129:8999\n",
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>77</td><td>application_1551772643782_0020</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://SYNPUNDT56SZ7R1:8088/proxy/application_1551772643782_0020/\">Link</a></td><td><a target=\"_blank\" href=\"http://SYNPUNDT56SZ7R1:8042/node/containerlogs/container_e15_1551772643782_0020_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "LivyUnexpectedStatusException",
     "evalue": "Session 77 unexpectedly reached final status 'error'. See logs:\nstdout: \n\nstderr: \nWarning: Master yarn-cluster is deprecated since 2.0. Please use master \"yarn\" with specified deploy mode instead.\n19/03/06 15:18:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n19/03/06 15:18:47 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n19/03/06 15:18:47 INFO RMProxy: Connecting to ResourceManager at SYNPUNDT56SZ7R1/10.34.14.129:8050\n19/03/06 15:18:47 INFO Client: Requesting a new application from cluster with 3 NodeManagers\n19/03/06 15:18:47 INFO Configuration: found resource resource-types.xml at file:/etc/hadoop/3.0.1.0-187/0/resource-types.xml\n19/03/06 15:18:47 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (3072 MB per container)\n19/03/06 15:18:47 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n19/03/06 15:18:47 INFO Client: Setting up container launch context for our AM\n19/03/06 15:18:47 INFO Client: Setting up the launch environment for our AM container\n19/03/06 15:18:47 INFO Client: Preparing resources for our AM container\n19/03/06 15:18:49 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://synpundt56sz7r1:8020/hdp/apps/3.0.1.0-187/spark2/spark2-hdp-yarn-archive.tar.gz\n19/03/06 15:18:49 INFO Client: Source and destination file systems are the same. Not copying hdfs://synpundt56sz7r1:8020/hdp/apps/3.0.1.0-187/spark2/spark2-hdp-yarn-archive.tar.gz\n19/03/06 15:18:49 INFO Client: Distribute hdfs cache file as spark.sql.hive.metastore.jars for HDP, hdfsCacheFile:hdfs://synpundt56sz7r1:8020/hdp/apps/3.0.1.0-187/spark2/spark2-hdp-hive-archive.tar.gz\n19/03/06 15:18:49 INFO Client: Source and destination file systems are the same. Not copying hdfs://synpundt56sz7r1:8020/hdp/apps/3.0.1.0-187/spark2/spark2-hdp-hive-archive.tar.gz\n19/03/06 15:18:49 INFO Client: Uploading resource file:/usr/hdp/current/livy2-server/rsc-jars/livy-rsc-0.5.0.3.0.1.0-187.jar -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/livy-rsc-0.5.0.3.0.1.0-187.jar\n19/03/06 15:18:49 INFO Client: Uploading resource file:/usr/hdp/current/livy2-server/rsc-jars/netty-all-4.0.37.Final.jar -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/netty-all-4.0.37.Final.jar\n19/03/06 15:18:50 INFO Client: Uploading resource file:/usr/hdp/current/livy2-server/rsc-jars/livy-api-0.5.0.3.0.1.0-187.jar -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/livy-api-0.5.0.3.0.1.0-187.jar\n19/03/06 15:18:50 INFO Client: Uploading resource file:/usr/hdp/current/livy2-server/repl_2.11-jars/livy-core_2.11-0.5.0.3.0.1.0-187.jar -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/livy-core_2.11-0.5.0.3.0.1.0-187.jar\n19/03/06 15:18:50 INFO Client: Uploading resource file:/usr/hdp/current/livy2-server/repl_2.11-jars/livy-repl_2.11-0.5.0.3.0.1.0-187.jar -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/livy-repl_2.11-0.5.0.3.0.1.0-187.jar\n19/03/06 15:18:50 INFO Client: Uploading resource file:/usr/hdp/current/livy2-server/repl_2.11-jars/commons-codec-1.9.jar -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/commons-codec-1.9.jar\n19/03/06 15:18:50 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/jars/datanucleus-api-jdo-3.2.6.jar -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/datanucleus-api-jdo-3.2.6.jar\n19/03/06 15:18:50 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/jars/datanucleus-core-3.2.10.jar -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/datanucleus-core-3.2.10.jar\n19/03/06 15:18:50 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/jars/datanucleus-rdbms-3.2.9.jar -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/datanucleus-rdbms-3.2.9.jar\n19/03/06 15:18:50 INFO Client: Uploading resource file:/etc/spark2/3.0.1.0-187/0/hive-site.xml -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/hive-site.xml\n19/03/06 15:18:50 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/R/lib/sparkr.zip#sparkr -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/sparkr.zip\n19/03/06 15:18:50 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/python/lib/pyspark.zip -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/pyspark.zip\n19/03/06 15:18:50 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/py4j-0.10.7-src.zip\n19/03/06 15:18:50 WARN Client: Same name resource file:///usr/hdp/current/spark2-client/python/lib/pyspark.zip added multiple times to distributed cache\n19/03/06 15:18:50 WARN Client: Same name resource file:///usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n19/03/06 15:18:51 INFO Client: Uploading resource file:/tmp/spark-cb783967-9faf-41b6-954a-22b9fc1996f2/__spark_conf__7083735340235196655.zip -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/__spark_conf__.zip\n19/03/06 15:18:51 INFO SecurityManager: Changing view acls to: livy\n19/03/06 15:18:51 INFO SecurityManager: Changing modify acls to: livy\n19/03/06 15:18:51 INFO SecurityManager: Changing view acls groups to: \n19/03/06 15:18:51 INFO SecurityManager: Changing modify acls groups to: \n19/03/06 15:18:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n19/03/06 15:18:51 INFO Client: Submitting application application_1551772643782_0020 to ResourceManager\n19/03/06 15:18:51 INFO YarnClientImpl: Submitted application application_1551772643782_0020\n19/03/06 15:18:51 INFO Client: Application report for application_1551772643782_0020 (state: ACCEPTED)\n19/03/06 15:18:51 INFO Client: \n\t client token: N/A\n\t diagnostics: [Wed Mar 06 15:18:51 +0530 2019] Application is Activated, waiting for resources to be assigned for AM.  Last Node which was processed for the application : SYNPUNDTL90:45454 ( Partition : [], Total resource : <memory:12288, vCores:2>, Available resource : <memory:4096, vCores:-2> ). Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:36864, vCores:6> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 38.88889 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:36864, vCores:6> ; Queue's used capacity (absolute resource) = <memory:14336, vCores:7> ; Queue's max capacity (absolute resource) = <memory:36864, vCores:6> ; \n\t ApplicationMaster host: N/A\n\t ApplicationMaster RPC port: -1\n\t queue: default\n\t start time: 1551865731224\n\t final status: UNDEFINED\n\t tracking URL: http://SYNPUNDT56SZ7R1:8088/proxy/application_1551772643782_0020/\n\t user: livy\n19/03/06 15:18:51 INFO ShutdownHookManager: Shutdown hook called\n19/03/06 15:18:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-d6fb056c-d58e-4535-99d8-2a921ce2632e\n19/03/06 15:18:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-cb783967-9faf-41b6-954a-22b9fc1996f2\n\nYARN Diagnostics: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLivyUnexpectedStatusException\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m/etc/Applications/Anaconda3/lib/python3.7/site-packages/hdijupyterutils/ipywidgetfactory.py\u001b[0m in \u001b[0;36msubmit_clicked\u001b[0;34m(self, button)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msubmit_clicked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbutton\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparent_widget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/etc/Applications/Anaconda3/lib/python3.7/site-packages/sparkmagic/controllerwidget/createsessionwidget.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_controller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             self.ipython_display.send_error(\"\"\"Could not add session with\n",
      "\u001b[0;32m/etc/Applications/Anaconda3/lib/python3.7/site-packages/sparkmagic/livyclientlib/sparkcontroller.py\u001b[0m in \u001b[0;36madd_session\u001b[0;34m(self, name, endpoint, skip_if_exists, properties)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_livy_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_client\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_display\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_session_id_for_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/etc/Applications/Anaconda3/lib/python3.7/site-packages/sparkmagic/livyclientlib/livysession.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCommand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sqlContext\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_display\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu\"SparkContext available as 'sc'.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/etc/Applications/Anaconda3/lib/python3.7/site-packages/sparkmagic/livyclientlib/command.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, session)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mstatement_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34mu\"code\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhttp_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_statement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/etc/Applications/Anaconda3/lib/python3.7/site-packages/sparkmagic/livyclientlib/livysession.py\u001b[0m in \u001b[0;36mwait_for_idle\u001b[0;34m(self, seconds_to_wait)\u001b[0m\n\u001b[1;32m    238\u001b[0m                     \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mLivyUnexpectedStatusException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'{} See logs:\\n{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mseconds_to_wait\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLivyUnexpectedStatusException\u001b[0m: Session 77 unexpectedly reached final status 'error'. See logs:\nstdout: \n\nstderr: \nWarning: Master yarn-cluster is deprecated since 2.0. Please use master \"yarn\" with specified deploy mode instead.\n19/03/06 15:18:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n19/03/06 15:18:47 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n19/03/06 15:18:47 INFO RMProxy: Connecting to ResourceManager at SYNPUNDT56SZ7R1/10.34.14.129:8050\n19/03/06 15:18:47 INFO Client: Requesting a new application from cluster with 3 NodeManagers\n19/03/06 15:18:47 INFO Configuration: found resource resource-types.xml at file:/etc/hadoop/3.0.1.0-187/0/resource-types.xml\n19/03/06 15:18:47 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (3072 MB per container)\n19/03/06 15:18:47 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\n19/03/06 15:18:47 INFO Client: Setting up container launch context for our AM\n19/03/06 15:18:47 INFO Client: Setting up the launch environment for our AM container\n19/03/06 15:18:47 INFO Client: Preparing resources for our AM container\n19/03/06 15:18:49 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://synpundt56sz7r1:8020/hdp/apps/3.0.1.0-187/spark2/spark2-hdp-yarn-archive.tar.gz\n19/03/06 15:18:49 INFO Client: Source and destination file systems are the same. Not copying hdfs://synpundt56sz7r1:8020/hdp/apps/3.0.1.0-187/spark2/spark2-hdp-yarn-archive.tar.gz\n19/03/06 15:18:49 INFO Client: Distribute hdfs cache file as spark.sql.hive.metastore.jars for HDP, hdfsCacheFile:hdfs://synpundt56sz7r1:8020/hdp/apps/3.0.1.0-187/spark2/spark2-hdp-hive-archive.tar.gz\n19/03/06 15:18:49 INFO Client: Source and destination file systems are the same. Not copying hdfs://synpundt56sz7r1:8020/hdp/apps/3.0.1.0-187/spark2/spark2-hdp-hive-archive.tar.gz\n19/03/06 15:18:49 INFO Client: Uploading resource file:/usr/hdp/current/livy2-server/rsc-jars/livy-rsc-0.5.0.3.0.1.0-187.jar -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/livy-rsc-0.5.0.3.0.1.0-187.jar\n19/03/06 15:18:49 INFO Client: Uploading resource file:/usr/hdp/current/livy2-server/rsc-jars/netty-all-4.0.37.Final.jar -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/netty-all-4.0.37.Final.jar\n19/03/06 15:18:50 INFO Client: Uploading resource file:/usr/hdp/current/livy2-server/rsc-jars/livy-api-0.5.0.3.0.1.0-187.jar -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/livy-api-0.5.0.3.0.1.0-187.jar\n19/03/06 15:18:50 INFO Client: Uploading resource file:/usr/hdp/current/livy2-server/repl_2.11-jars/livy-core_2.11-0.5.0.3.0.1.0-187.jar -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/livy-core_2.11-0.5.0.3.0.1.0-187.jar\n19/03/06 15:18:50 INFO Client: Uploading resource file:/usr/hdp/current/livy2-server/repl_2.11-jars/livy-repl_2.11-0.5.0.3.0.1.0-187.jar -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/livy-repl_2.11-0.5.0.3.0.1.0-187.jar\n19/03/06 15:18:50 INFO Client: Uploading resource file:/usr/hdp/current/livy2-server/repl_2.11-jars/commons-codec-1.9.jar -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/commons-codec-1.9.jar\n19/03/06 15:18:50 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/jars/datanucleus-api-jdo-3.2.6.jar -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/datanucleus-api-jdo-3.2.6.jar\n19/03/06 15:18:50 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/jars/datanucleus-core-3.2.10.jar -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/datanucleus-core-3.2.10.jar\n19/03/06 15:18:50 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/jars/datanucleus-rdbms-3.2.9.jar -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/datanucleus-rdbms-3.2.9.jar\n19/03/06 15:18:50 INFO Client: Uploading resource file:/etc/spark2/3.0.1.0-187/0/hive-site.xml -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/hive-site.xml\n19/03/06 15:18:50 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/R/lib/sparkr.zip#sparkr -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/sparkr.zip\n19/03/06 15:18:50 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/python/lib/pyspark.zip -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/pyspark.zip\n19/03/06 15:18:50 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/py4j-0.10.7-src.zip\n19/03/06 15:18:50 WARN Client: Same name resource file:///usr/hdp/current/spark2-client/python/lib/pyspark.zip added multiple times to distributed cache\n19/03/06 15:18:50 WARN Client: Same name resource file:///usr/hdp/current/spark2-client/python/lib/py4j-0.10.7-src.zip added multiple times to distributed cache\n19/03/06 15:18:51 INFO Client: Uploading resource file:/tmp/spark-cb783967-9faf-41b6-954a-22b9fc1996f2/__spark_conf__7083735340235196655.zip -> hdfs://synpundt56sz7r1:8020/user/livy/.sparkStaging/application_1551772643782_0020/__spark_conf__.zip\n19/03/06 15:18:51 INFO SecurityManager: Changing view acls to: livy\n19/03/06 15:18:51 INFO SecurityManager: Changing modify acls to: livy\n19/03/06 15:18:51 INFO SecurityManager: Changing view acls groups to: \n19/03/06 15:18:51 INFO SecurityManager: Changing modify acls groups to: \n19/03/06 15:18:51 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(livy); groups with view permissions: Set(); users  with modify permissions: Set(livy); groups with modify permissions: Set()\n19/03/06 15:18:51 INFO Client: Submitting application application_1551772643782_0020 to ResourceManager\n19/03/06 15:18:51 INFO YarnClientImpl: Submitted application application_1551772643782_0020\n19/03/06 15:18:51 INFO Client: Application report for application_1551772643782_0020 (state: ACCEPTED)\n19/03/06 15:18:51 INFO Client: \n\t client token: N/A\n\t diagnostics: [Wed Mar 06 15:18:51 +0530 2019] Application is Activated, waiting for resources to be assigned for AM.  Last Node which was processed for the application : SYNPUNDTL90:45454 ( Partition : [], Total resource : <memory:12288, vCores:2>, Available resource : <memory:4096, vCores:-2> ). Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:36864, vCores:6> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 38.88889 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:36864, vCores:6> ; Queue's used capacity (absolute resource) = <memory:14336, vCores:7> ; Queue's max capacity (absolute resource) = <memory:36864, vCores:6> ; \n\t ApplicationMaster host: N/A\n\t ApplicationMaster RPC port: -1\n\t queue: default\n\t start time: 1551865731224\n\t final status: UNDEFINED\n\t tracking URL: http://SYNPUNDT56SZ7R1:8088/proxy/application_1551772643782_0020/\n\t user: livy\n19/03/06 15:18:51 INFO ShutdownHookManager: Shutdown hook called\n19/03/06 15:18:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-d6fb056c-d58e-4535-99d8-2a921ce2632e\n19/03/06 15:18:51 INFO ShutdownHookManager: Deleting directory /tmp/spark-cb783967-9faf-41b6-954a-22b9fc1996f2\n\nYARN Diagnostics: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted session 76 at http://10.34.14.129:8999\n"
     ]
    },
    {
     "ename": "HttpClientException",
     "evalue": "Invalid status code '404' from http://10.34.14.129:8999/sessions/77 with error payload: \"Session '77' not found.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpClientException\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/etc/Applications/Anaconda3/lib/python3.7/site-packages/sparkmagic/controllerwidget/manageendpointwidget.py\u001b[0m in \u001b[0;36mdelete_endpoint\u001b[0;34m(button)\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_controller\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete_session_by_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mipython_display\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteln\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Deleted session {} at {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/etc/Applications/Anaconda3/lib/python3.7/site-packages/sparkmagic/livyclientlib/sparkcontroller.py\u001b[0m in \u001b[0;36mdelete_session_by_id\u001b[0;34m(self, endpoint, session_id)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mhttp_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_http_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mhttp_client\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_http_client\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             session = self._livy_session(http_client, {constants.LIVY_KIND_PARAM: response[constants.LIVY_KIND_PARAM]},\n",
      "\u001b[0;32m/etc/Applications/Anaconda3/lib/python3.7/site-packages/sparkmagic/livyclientlib/livyreliablehttpclient.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m(self, session_id)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_http_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdelete_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/etc/Applications/Anaconda3/lib/python3.7/site-packages/sparkmagic/livyclientlib/reliablehttpclient.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, relative_url, accepted_status_codes)\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted_status_codes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;34m\"\"\"Sends a get request. Returns a response.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelative_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted_status_codes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted_status_codes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/etc/Applications/Anaconda3/lib/python3.7/site-packages/sparkmagic/livyclientlib/reliablehttpclient.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, relative_url, accepted_status_codes, function, data)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted_status_codes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompose_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrelative_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted_status_codes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_request_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccepted_status_codes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/etc/Applications/Anaconda3/lib/python3.7/site-packages/sparkmagic/livyclientlib/reliablehttpclient.py\u001b[0m in \u001b[0;36m_send_request_helper\u001b[0;34m(self, url, accepted_status_codes, function, data, retry_count)\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     raise HttpClientException(u\"Invalid status code '{}' from {} with error payload: {}\"\n\u001b[0;32m---> 95\u001b[0;31m                                               .format(status, url, text))\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHttpClientException\u001b[0m: Invalid status code '404' from http://10.34.14.129:8999/sessions/77 with error payload: \"Session '77' not found.\""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added endpoint http://10.34.14.129:8999\n",
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>78</td><td>application_1551772643782_0021</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://SYNPUNDT56SZ7R1:8088/proxy/application_1551772643782_0021/\">Link</a></td><td><a target=\"_blank\" href=\"http://SYNPUNDT3022488:8042/node/containerlogs/container_e15_1551772643782_0021_01_000001/livy\">Link</a></td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "%load_ext sparkmagic.magics\n",
    "%manage_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T09:53:48.005033Z",
     "start_time": "2019-03-06T09:53:47.968016Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.conf.SparkConf object at 0x7f041ae7f950>"
     ]
    }
   ],
   "source": [
    "spark.sparkContext._conf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T10:02:13.182894Z",
     "start_time": "2019-03-06T10:01:39.731403Z"
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.load(\"/Fannie-Mae/2017/Output.csv/part*\", format=\"csv\")\n",
    "\n",
    "df.write.parquet(\"/Fannie-Mae/2017/ParquetResult_2017.parquet\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-06T10:02:59.010241Z",
     "start_time": "2019-03-06T10:02:56.724633Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+--------------------+-----+------+---+-------+-------+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+-----+---------+----+-----+-----+-------+-----+----+----+----+-------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----------+\n",
      "|         _c0|_c1|                 _c2|  _c3|   _c4|_c5|    _c6|    _c7| _c8| _c9|_c10|_c11| _c12|_c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20|_c21| _c22|_c23|_c24|_c25| _c26|     _c27|_c28| _c29| _c30|   _c31| _c32|_c33|_c34|_c35|   _c36|_c37|_c38|_c39|_c40|_c41|_c42|_c43|_c44|_c45|_c46|_c47|_c48|_c49|_c50|_c51|_c52|_c53|      _c54|\n",
      "+------------+---+--------------------+-----+------+---+-------+-------+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+-----+---------+----+-----+-----+-------+-----+----+----+----+-------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----------+\n",
      "|100085604535|  C|WELLS FARGO BANK,...|4.125|338000|360|09/2017|11/2017|90.0|90.0| 2.0|36.0|764.0|   N|   P|  SF|   1|   S|  TX| 778|25.0| FRM|727.0| 1.0|   N|null|4.125|331684.47|  11|349.0|348.0|10/2047|    0|   0|   N|null|   null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|   N|2018-09-01|\n",
      "|100163185387|  R|      U.S. BANK N.A.|  4.0|214000|360|05/2017|07/2017|80.0|80.0| 2.0|23.0|800.0|   N|   P|  SF|   1|   P|  TN| 373|null| FRM|770.0|null|   N|null|  4.0| 197725.6|  15|345.0|313.0|06/2047|    0|   0|   N|null|   null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|   N|2018-09-01|\n",
      "|100397003615|  C|WELLS FARGO BANK,...| 4.25|215000|360|05/2017|07/2017|78.0|78.0| 1.0|39.0|767.0|   N|   P|  SF|   1|   P|  PA| 189|null| FRM| null|null|   N|null| 4.25|209643.94|  15|345.0|344.0|06/2047|37980|   0|   N|null|   null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|   N|2018-09-01|\n",
      "|100427862701|  C|  FLAGSTAR BANK, FSB|4.125|225000|360|09/2017|12/2017|60.0|60.0| 1.0|38.0|823.0|   N|   C|  SF|   1|   P|  CA| 959|null| FRM| null|null|   N|null|4.125|221780.28|  10|350.0|350.0|11/2047|17020|   0|   N|null|   null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|   N|2018-09-01|\n",
      "|100566818589|  C|WELLS FARGO BANK,...| 3.25|123000|180|05/2017|07/2017|80.0|80.0| 2.0|17.0|782.0|   N|   P|  PU|   1|   S|  AR| 727|null| FRM|791.0|null|   N|null| 3.25|115066.58|  15|165.0|165.0|06/2032|22220|   0|   N|null|   null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|   N|2018-09-01|\n",
      "|100695008116|  R|               OTHER|4.125|324000|360|05/2017|07/2017|92.0|92.0| 2.0|28.0|750.0|   N|   P|  PU|   1|   P|  OH| 430|30.0| FRM|742.0| 1.0|   N|null|4.125|315733.22|  15|345.0|343.0|06/2047|18140|   0|   N|null|   null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|   N|2018-09-01|\n",
      "|100779848132|  C|AMERIHOME MORTGAG...| 4.25|301000|360|09/2017|11/2017|67.0|67.0| 2.0|31.0|703.0|   N|   C|  SF|   1|   P|  CA| 920|null| FRM|692.0|null|   N|null| 4.25| 296356.7|  11|349.0|349.0|10/2047|41740|   0|   N|null|   null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|   N|2018-09-01|\n",
      "|100864132236|  R|               OTHER|  5.0|280000|360|05/2017|07/2017|80.0|80.0| 1.0|35.0|703.0|   N|   P|  SF|   1|   I|  CT|  69|null| FRM| null|null|   N|null|  5.0|277960.25|   7|353.0|  0.0|06/2047|14860|   X|   N| 1.0|01/2018|null|null|null|null|null|null|null|null|null|null|null|null|null|null|   N|null|null|2018-01-01|\n",
      "|100995160644|  C|       SUNTRUST BANK|3.875|374000|360|07/2017|09/2017|94.0|94.0| 2.0|46.0|755.0|   N|   R|  PU|   1|   P|  MT| 597|30.0| FRM|779.0| 1.0|   N|null|3.875| 366696.8|  13|347.0|347.0|08/2047|    0|   0|   N|null|   null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|   N|2018-09-01|\n",
      "|101129005555|  R|  QUICKEN LOANS INC.|  4.5|128000|360|03/2017|05/2017|79.0|79.0| 1.0|44.0|734.0|   N|   C|  SF|   1|   P|  OR| 974|null| FRM| null|null|   N|null|  4.5|121508.25|  17|343.0|324.0|04/2047|    0|   0|   N|null|   null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|null|   N|2018-09-01|\n",
      "+------------+---+--------------------+-----+------+---+-------+-------+----+----+----+----+-----+----+----+----+----+----+----+----+----+----+-----+----+----+----+-----+---------+----+-----+-----+-------+-----+----+----+----+-------+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "df2 = spark.read.parquet(\"/Fannie-Mae/2017/ParquetResult_2017.parquet/part*\")\n",
    "\n",
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
